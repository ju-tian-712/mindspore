layer_norm_ext:
  description: |
    Applies the Layer Normalization to the input tensor.

    This operator will normalize the input tensor on given axis. LayerNorm is described in the paper
    `Layer Normalization <https://arxiv.org/abs/1607.06450>`_.

    .. math::
        y = \frac{x - mean}{\sqrt{variance + \epsilon}} * \gamma + \beta

    where :math:`\gamma` is weight, :math:`\beta` is bias, :math:`\epsilon` is eps.

    Args:
        input_x (Tensor): Tensor of shape :math:`(N, \ldots)`. The input of LayerNorm.
        normalized_shape (Union(tuple[int], list[int])): The normalized shape of `input_x` for LayerNorm.
        weight (Tensor, optional): Learnable parameter :math:`\gamma` . Tensor of shape `normalized_shape`. Default: ``None`` .
        bias (Tensor, optional): Learnable parameter :math:`\beta` . Tensor of shape `normalized_shape`. Default: ``None`` .
        eps (float, optional): A value added to the denominator for numerical stability(:math:`\epsilon`). Default: ``1e-5`` .

    Returns:
        tuple[Tensor], tuple of 3 tensors, the normalized input and the updated parameters.

        - **output_x** (Tensor) - The normalized input, has the same type and shape as the `input_x`.
        - **mean** (Tensor) - The first `begin_norm_axis` (The begin axis of the `input_x` to apply LayerNorm) dimensions of `mean` shape is the same as `input_x`,
          and the remaining dimensions are 1. Suppose the shape of the `input_x` is :math:`(x_1, x_2, \ldots, x_R)`,
          the shape of the `mean` is :math:`(x_1, \ldots, x_{begin_params_axis}, 1, \ldots, 1)`
          (when `begin_params_axis=0`, the shape of `mean` is :math:`(1, \ldots, 1)` ).
        - **rstd** (Tensor) - Shape is the same as `mean` .

    Raises:
        TypeError: If `eps` is not a float.
        TypeError: If `input_x`, `weight` or `bias` is not a Tensor.

    Supported Platforms:
        ``Ascend`` ``GPU`` ``CPU``

    Examples:
        >>> import mindspore
        >>> import numpy as np
        >>> from mindspore import Tensor, ops
        >>> input_x = Tensor(np.array([[1, 2, 3], [1, 2, 3]]), mindspore.float32)
        >>> normalized_shape = (3,)
        >>> gamma = Tensor(np.ones(normalized_shape), mindspore.float32)
        >>> beta = Tensor(np.zeros(normalized_shape), mindspore.float32)
        >>> eps = 1e-7
        >>> layer_norm = ops.LayerNormExt()
        >>> output, mean, rstd = layer_norm(input_x, normalized_shape, gamma, beta, eps)
        >>> print(output)
        [[-1.2247448  1.         1.2247448]
         [-1.2247448  1.         1.2247448]]
        >>> print(mean)
        [[2.]
         [2.]]
        >>> print(rstd)
        [[1.2247447]
         [1.2247447]]